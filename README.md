### **Part 1 — Baseline Coverage**
- Used the **HumanEval dataset** with programs generated by **DeepSeek** and **LLaMA3** models using both CoT and Self-Debugging strategies.  
- Wrote a Python script (`generate_tests.py`) to automatically create baseline test cases for each program.  
- Executed tests using **pytest** with coverage reporting and stored outputs in XML format.  
- Summarized results using `coverage_summary.py`, which includes:  
  - Number of tests passed vs. total tests  
  - Line and branch coverage (%)  
  - Recommendations for improving coverage  

**Output:** Reports are stored in the `coverage_reports_assignment1` folder.

---

### **Part 2 — LLM-Assisted Test Generation & Coverage Improvement**
- Selected two new Python programs:  
  1. `inventory_pricing_analysis.py`  
  2. `student_grade_analysis.py`  
- Used **LLM-generated test cases** iteratively across four rounds to improve coverage.  
- Each iteration introduced more complex or boundary-focused tests to improve **line and branch coverage**.  
- Implemented a **test de-duplication strategy** to remove redundant test cases while maintaining full coverage.

**Results:**  
Both programs achieved **100% line and branch coverage** through iterative LLM-driven test generation and refinement.
