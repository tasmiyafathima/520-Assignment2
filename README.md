### **Part 1 â€” Baseline Coverage**
- Used the **HumanEval dataset** with programs generated by **DeepSeek** and **LLaMA3** models using both CoT and Self-Debugging strategies.  
- Wrote a Python script (`generate_tests.py`) to automatically create baseline test cases for each program from the HumanEval Dataset which was used in assignment 1 - new_data.json  
- Executed tests using **pytest** with coverage reporting using the below command and stored outputs in XML format.  
  PYTHONPATH=$(pwd) pytest -v tests/deepseek_CoT \
  --junitxml=reports/junit_all_deepseek_CoT.xml \
  --cov=outputs.deepseek_CoT \
  --cov-report=xml:reports/coverage_all_deepseek_CoT.xml \
  --cov-branch > reports/deepseek_CoT_output.txt
  
- Summarized results using `coverage_summary.py`, which includes:  
  - Number of tests passed vs. total tests  
  - Line and branch coverage (%)  
  - Recommendations for improving coverage  

**Output:** Reports are stored in the `coverage_reports_assignment1` folder.

---

### **Part 2 â€” LLM-Assisted Test Generation & Coverage Improvement**
- Selected two new Python programs:  
  1. `inventory_pricing_analysis.py`  
  2. `student_grade_analysis.py`  
- Used **LLM-generated test cases** iteratively across four rounds to improve coverage.  
- Each iteration introduced more complex or boundary-focused tests to improve **line and branch coverage**.  
- Implemented a **test de-duplication strategy** to remove redundant test cases while maintaining full coverage.

**Results:**  
Both programs achieved **100% line and branch coverage** through iterative LLM-driven test generation and refinement.
---

### **Part 3 â€” Fault Detection Check**
This phase evaluated whether high coverage also translated into **effective bug detection**. To test this, intentional faults were introduced into both programs.

#### ðŸ”¹ Program 1: `inventory_pricing_analysis.py`
- **Bug Introduced:** *Boundary misclassification (off-by-one)* â€” items priced exactly at 50 were incorrectly classified as Discount instead of Standard.
- **Test Suite Behavior:**
  - Expanded tests to validate both **item names and categories**.
  - Included test cases for all branches: Premium (>100), Standard (â‰¥50), Discount (<50), and Out-of-stock items.
- **Results:**
  - The bug was successfully detected by boundary-specific tests.
  - Demonstrated that high coverage alone is not sufficient â€” targeted edge-case testing is crucial.
- **Insight:**
  - Careful design of boundary and classification tests significantly enhances fault detection capability.

#### ðŸ”¹ Program 2: `student_grade_analysis.py`
- **Bug Introduced:** *Off-by-one error* â€” students scoring exactly 70 or 90 were misclassified (Merit and Distinction thresholds shifted).
- **Test Suite Behavior:**
  - Tests covered all categories: Distinction (â‰¥90), Merit (â‰¥70), Pass (â‰¥50), and Fail (below thresholds).
  - Special boundary cases validated classification accuracy for exact threshold scores.
- **Results:**
  - The test suite correctly detected the misclassification errors.
  - Reinforced that coverage metrics must be complemented by **boundary-specific validation**.
- **Insight:**
  - Even with 100% coverage, missing edge-case scenarios can lead to undetected bugs.

---
